{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"0c044dd9-293a-4185-9ae6-6b8e89b4b943","_uuid":"9779523e-d5d6-41f0-a337-018ba58069b8","collapsed":false,"execution":{"iopub.execute_input":"2024-04-24T12:58:06.846927Z","iopub.status.busy":"2024-04-24T12:58:06.846576Z","iopub.status.idle":"2024-04-24T13:01:44.377841Z","shell.execute_reply":"2024-04-24T13:01:44.376816Z","shell.execute_reply.started":"2024-04-24T12:58:06.846902Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"]},{"name":"stdout","output_type":"stream","text":["| epoch   1 |   767/ 3839 batches | lr 0.005000 |  4.88 ms | loss 0.17508 | ppl     1.19\n","| epoch   1 |  1534/ 3839 batches | lr 0.005000 |  4.79 ms | loss 0.20081 | ppl     1.22\n","| epoch   1 |  2301/ 3839 batches | lr 0.005000 |  4.82 ms | loss 0.03487 | ppl     1.04\n","| epoch   1 |  3068/ 3839 batches | lr 0.005000 |  5.12 ms | loss 0.05781 | ppl     1.06\n","| epoch   1 |  3835/ 3839 batches | lr 0.005000 |  4.99 ms | loss 0.06084 | ppl     1.06\n","-----------------------------------------------------------------------------------------\n","| end of epoch   1 | time: 20.01s | valid loss 0.07597 | valid ppl     1.08\n","-----------------------------------------------------------------------------------------\n","| epoch   2 |   767/ 3839 batches | lr 0.004513 |  4.85 ms | loss 0.11239 | ppl     1.12\n","| epoch   2 |  1534/ 3839 batches | lr 0.004513 |  4.74 ms | loss 0.20527 | ppl     1.23\n","| epoch   2 |  2301/ 3839 batches | lr 0.004513 |  4.77 ms | loss 0.03574 | ppl     1.04\n","| epoch   2 |  3068/ 3839 batches | lr 0.004513 |  4.72 ms | loss 0.05624 | ppl     1.06\n","| epoch   2 |  3835/ 3839 batches | lr 0.004513 |  4.70 ms | loss 0.05964 | ppl     1.06\n","-----------------------------------------------------------------------------------------\n","| end of epoch   2 | time: 19.32s | valid loss 0.07628 | valid ppl     1.08\n","-----------------------------------------------------------------------------------------\n","| epoch   3 |   767/ 3839 batches | lr 0.004287 |  4.68 ms | loss 0.11253 | ppl     1.12\n","| epoch   3 |  1534/ 3839 batches | lr 0.004287 |  4.65 ms | loss 0.20052 | ppl     1.22\n","| epoch   3 |  2301/ 3839 batches | lr 0.004287 |  4.63 ms | loss 0.03701 | ppl     1.04\n","| epoch   3 |  3068/ 3839 batches | lr 0.004287 |  4.63 ms | loss 0.05997 | ppl     1.06\n","| epoch   3 |  3835/ 3839 batches | lr 0.004287 |  4.71 ms | loss 0.06050 | ppl     1.06\n","-----------------------------------------------------------------------------------------\n","| end of epoch   3 | time: 18.91s | valid loss 0.08541 | valid ppl     1.09\n","-----------------------------------------------------------------------------------------\n","| epoch   4 |   767/ 3839 batches | lr 0.004073 |  4.65 ms | loss 0.11112 | ppl     1.12\n","| epoch   4 |  1534/ 3839 batches | lr 0.004073 |  4.66 ms | loss 0.20394 | ppl     1.23\n","| epoch   4 |  2301/ 3839 batches | lr 0.004073 |  4.63 ms | loss 0.03614 | ppl     1.04\n","| epoch   4 |  3068/ 3839 batches | lr 0.004073 |  4.62 ms | loss 0.06082 | ppl     1.06\n","| epoch   4 |  3835/ 3839 batches | lr 0.004073 |  4.67 ms | loss 0.06184 | ppl     1.06\n","-----------------------------------------------------------------------------------------\n","| end of epoch   4 | time: 18.89s | valid loss 0.07887 | valid ppl     1.08\n","-----------------------------------------------------------------------------------------\n","| epoch   5 |   767/ 3839 batches | lr 0.003869 |  4.65 ms | loss 0.11167 | ppl     1.12\n","| epoch   5 |  1534/ 3839 batches | lr 0.003869 |  4.64 ms | loss 0.20274 | ppl     1.22\n","| epoch   5 |  2301/ 3839 batches | lr 0.003869 |  4.69 ms | loss 0.03660 | ppl     1.04\n","| epoch   5 |  3068/ 3839 batches | lr 0.003869 |  4.67 ms | loss 0.06100 | ppl     1.06\n","| epoch   5 |  3835/ 3839 batches | lr 0.003869 |  4.67 ms | loss 0.06093 | ppl     1.06\n","-----------------------------------------------------------------------------------------\n","| end of epoch   5 | time: 29.26s | valid loss 0.07988 | valid ppl     1.08\n","-----------------------------------------------------------------------------------------\n","| epoch   6 |   767/ 3839 batches | lr 0.003675 |  4.79 ms | loss 0.11121 | ppl     1.12\n","| epoch   6 |  1534/ 3839 batches | lr 0.003675 |  4.62 ms | loss 0.20093 | ppl     1.22\n","| epoch   6 |  2301/ 3839 batches | lr 0.003675 |  4.68 ms | loss 0.03619 | ppl     1.04\n","| epoch   6 |  3068/ 3839 batches | lr 0.003675 |  4.69 ms | loss 0.06032 | ppl     1.06\n","| epoch   6 |  3835/ 3839 batches | lr 0.003675 |  4.70 ms | loss 0.06175 | ppl     1.06\n","-----------------------------------------------------------------------------------------\n","| end of epoch   6 | time: 19.10s | valid loss 0.07718 | valid ppl     1.08\n","-----------------------------------------------------------------------------------------\n","| epoch   7 |   767/ 3839 batches | lr 0.003492 |  4.70 ms | loss 0.11119 | ppl     1.12\n","| epoch   7 |  1534/ 3839 batches | lr 0.003492 |  4.67 ms | loss 0.20619 | ppl     1.23\n","| epoch   7 |  2301/ 3839 batches | lr 0.003492 |  4.66 ms | loss 0.03814 | ppl     1.04\n","| epoch   7 |  3068/ 3839 batches | lr 0.003492 |  4.65 ms | loss 0.05911 | ppl     1.06\n","| epoch   7 |  3835/ 3839 batches | lr 0.003492 |  4.63 ms | loss 0.05995 | ppl     1.06\n","-----------------------------------------------------------------------------------------\n","| end of epoch   7 | time: 18.93s | valid loss 0.07618 | valid ppl     1.08\n","-----------------------------------------------------------------------------------------\n","| epoch   8 |   767/ 3839 batches | lr 0.003317 |  4.64 ms | loss 0.11121 | ppl     1.12\n","| epoch   8 |  1534/ 3839 batches | lr 0.003317 |  4.67 ms | loss 0.20802 | ppl     1.23\n","| epoch   8 |  2301/ 3839 batches | lr 0.003317 |  4.61 ms | loss 0.03822 | ppl     1.04\n","| epoch   8 |  3068/ 3839 batches | lr 0.003317 |  4.61 ms | loss 0.05906 | ppl     1.06\n","| epoch   8 |  3835/ 3839 batches | lr 0.003317 |  4.61 ms | loss 0.06131 | ppl     1.06\n","-----------------------------------------------------------------------------------------\n","| end of epoch   8 | time: 18.80s | valid loss 0.07540 | valid ppl     1.08\n","-----------------------------------------------------------------------------------------\n","| epoch   9 |   767/ 3839 batches | lr 0.003151 |  4.63 ms | loss 0.11069 | ppl     1.12\n","| epoch   9 |  1534/ 3839 batches | lr 0.003151 |  4.61 ms | loss 0.20526 | ppl     1.23\n","| epoch   9 |  2301/ 3839 batches | lr 0.003151 |  4.62 ms | loss 0.03811 | ppl     1.04\n","| epoch   9 |  3068/ 3839 batches | lr 0.003151 |  4.64 ms | loss 0.05947 | ppl     1.06\n","| epoch   9 |  3835/ 3839 batches | lr 0.003151 |  4.73 ms | loss 0.06268 | ppl     1.06\n","-----------------------------------------------------------------------------------------\n","| end of epoch   9 | time: 18.87s | valid loss 0.07586 | valid ppl     1.08\n","-----------------------------------------------------------------------------------------\n","| epoch  10 |   767/ 3839 batches | lr 0.002994 |  4.62 ms | loss 0.11127 | ppl     1.12\n","| epoch  10 |  1534/ 3839 batches | lr 0.002994 |  4.62 ms | loss 0.20057 | ppl     1.22\n","| epoch  10 |  2301/ 3839 batches | lr 0.002994 |  4.61 ms | loss 0.03839 | ppl     1.04\n","| epoch  10 |  3068/ 3839 batches | lr 0.002994 |  4.65 ms | loss 0.06044 | ppl     1.06\n","| epoch  10 |  3835/ 3839 batches | lr 0.002994 |  4.60 ms | loss 0.06179 | ppl     1.06\n","-----------------------------------------------------------------------------------------\n","| end of epoch  10 | time: 29.14s | valid loss 0.07368 | valid ppl     1.08\n","-----------------------------------------------------------------------------------------\n"]}],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import time\n","import math\n","from matplotlib import pyplot\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","\n","torch.manual_seed(0)\n","np.random.seed(0)\n","\n","# S is the source sequence length\n","# T is the target sequence length\n","# N is the batch size\n","# E is the feature number\n","\n","#src = torch.rand((10, 32, 512)) # (S,N,E) \n","#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n","#out = transformer_model(src, tgt)\n","\n","input_window = 100 # number of input steps\n","output_window = 1 # number of prediction steps, in this model its fixed to one\n","block_len = input_window + output_window # for one input-output pair\n","batch_size = 10\n","train_size = 0.8\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()       \n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        # div_term = torch.exp(\n","        #     torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n","        # )\n","        div_term = 1 / (10000 ** ((2 * np.arange(d_model)) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term[0::2])\n","        pe[:, 1::2] = torch.cos(position * div_term[1::2])\n","\n","        pe = pe.unsqueeze(0).transpose(0, 1) # [5000, 1, d_model],so need seq-len <= 5000\n","        #pe.requires_grad = False\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # print(self.pe[:x.size(0), :].repeat(1,x.shape[1],1).shape ,'---',x.shape)\n","        # dimension 1 maybe inequal batchsize\n","        return x + self.pe[:x.size(0), :].repeat(1,x.shape[1],1)\n","          \n","\n","import torch.nn as nn\n","import torch\n","\n","class JV2formerWithRNN(nn.Module):\n","    def __init__(self, feature_size=250, num_layers=1, dropout=0.1):\n","        super(JV2formerWithRNN, self).__init__()\n","        self.model_type = 'Transformer with RNN'\n","        \n","        # Transformer Encoder\n","        self.input_embedding = nn.Linear(1, feature_size)\n","        self.pos_encoder = PositionalEncoding(feature_size)\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n","        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","        \n","        # RNN (GRU and LSTM)\n","        self.gru = nn.GRU(input_size=feature_size, hidden_size=feature_size, num_layers=1, batch_first=True)\n","        self.lstm = nn.LSTM(input_size=feature_size, hidden_size=feature_size, num_layers=1, batch_first=True)\n","        \n","        # Linear layer for output\n","        self.decoder = nn.Linear(feature_size * 3, 1)  # Concatenate transformer, GRU, and LSTM outputs\n","        \n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.1    \n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src):\n","        # src with shape (input_window, batch_len, 1)\n","        if self.src_mask is None or self.src_mask.size(0) != len(src):\n","            device = src.device\n","            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n","            self.src_mask = mask\n","\n","        # Transformer Encoder\n","        src_transformer = self.input_embedding(src)\n","        src_transformer = self.pos_encoder(src_transformer)\n","        output_transformer = self.transformer_encoder(src_transformer, self.src_mask)\n","\n","        # GRU\n","        _, hidden_gru = self.gru(src_transformer)\n","        output_gru = hidden_gru[-1]  # Taking the last layer's output\n","        \n","        # LSTM\n","        _, (hidden_lstm, _) = self.lstm(src_transformer)\n","        output_lstm = hidden_lstm[-1]  # Taking the last layer's output\n","        \n","        # Concatenate outputs from Transformer, GRU, and LSTM\n","        output_combined = torch.cat((output_transformer[-1], output_gru, output_lstm), dim=1)\n","        \n","        # Linear layer for final output\n","        output = self.decoder(output_combined)\n","        return output\n","\n","\n","    def _generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","# if window is 100 and prediction step is 1\n","# in -> [0..99]\n","# target -> [1..100]\n","'''\n","In fact, assuming that the number of samples is N, \n","the length of the input sequence is m, and the backward prediction is k steps, \n","then length of a block [input : 1 , 2 ... m  -> output : k , k+1....m+k ] \n","should be (m+k) :  block_len, so to ensure that each block is complete, \n","the end element of the last block should be the end element of the entire sequence, \n","so the actual number of blocks is [N - block_len + 1] \n","'''\n","def create_inout_sequences(input_data, input_window ,output_window):\n","    inout_seq = []\n","    L = len(input_data)\n","    block_num =  L - block_len + 1\n","    # total of [N - block_len + 1] blocks\n","    # where block_len = input_window + output_window\n","\n","    for i in range( block_num ):\n","        train_seq = input_data[i : i + input_window]\n","        train_label = input_data[i + output_window : i + input_window + output_window]\n","        inout_seq.append((train_seq ,train_label))\n","\n","    return torch.FloatTensor(np.array(inout_seq))\n","\n","def get_data(csv_file, train_size, input_window, output_window, device):\n","    # Read CSV file into a DataFrame\n","    data = pd.read_csv(csv_file)\n","\n","    # Convert DateTime column to pandas DateTime object\n","    data['DateTime'] = pd.to_datetime(data['DateTime'])\n","\n","    # Normalize vehicle count column\n","    scaler = MinMaxScaler(feature_range=(-1, 1)) \n","    data['Vehicles'] = scaler.fit_transform(data['Vehicles'].values.reshape(-1, 1)).reshape(-1)\n","\n","    # Split data into train and test sets based on train_size\n","    sampels = int(len(data) * train_size)\n","    train_data = data['Vehicles'][:sampels]\n","    test_data = data['Vehicles'][sampels:]\n","\n","    # Create input-output sequences for training and testing sets\n","    train_sequence = create_inout_sequences(train_data, input_window, output_window)\n","    test_sequence = create_inout_sequences(test_data, input_window, output_window)\n","\n","    # Convert sequences to PyTorch tensors and move to device\n","    train_sequence = torch.FloatTensor(train_sequence).to(device)\n","    test_sequence = torch.FloatTensor(test_sequence).to(device)\n","\n","    return train_sequence, test_sequence\n","\n","\n","def get_batch(input_data, i , batch_size):\n","\n","    # batch_len = min(batch_size, len(input_data) - 1 - i) #  # Now len-1 is not necessary\n","    batch_len = min(batch_size, len(input_data) - i)\n","    data = input_data[ i:i + batch_len ]\n","    input = torch.stack([item[0] for item in data]).view((input_window,batch_len,1))\n","    # ( seq_len, batch, 1 ) , 1 is feature size\n","    target = torch.stack([item[1] for item in data]).view((input_window,batch_len,1))\n","    return input, target\n","\n","def train(train_data):\n","    model.train() # Turn on the train mode \\o/\n","    total_loss = 0.\n","    start_time = time.time()\n","\n","    for batch, i in enumerate(range(0, len(train_data), batch_size)):  # Now len-1 is not necessary\n","        # data and target are the same shape with (input_window,batch_len,1)\n","        data, targets = get_batch(train_data, i , batch_size)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, targets)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        log_interval = int(len(train_data) / batch_size / 5)\n","        if batch % log_interval == 0 and batch > 0:\n","            cur_loss = total_loss / log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n","                  'lr {:02.6f} | {:5.2f} ms | '\n","                  'loss {:5.5f} | ppl {:8.2f}'.format(\n","                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n","                    elapsed * 1000 / log_interval,\n","                    cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","\n","def plot_and_loss(eval_model, data_source,epoch):\n","    eval_model.eval() \n","    total_loss = 0.\n","    test_result = torch.Tensor(0)    \n","    truth = torch.Tensor(0)\n","    with torch.no_grad():\n","        # for i in range(0, len(data_source) - 1):\n","        for i in range(len(data_source)):  # Now len-1 is not necessary\n","            data, target = get_batch(data_source, i , 1) # one-step forecast\n","            output = eval_model(data)            \n","            total_loss += criterion(output, target).item()\n","            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0)\n","            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n","            \n","    #test_result = test_result.cpu().numpy() -> no need to detach stuff.. \n","    len(test_result)\n","\n","    pyplot.plot(test_result,color=\"red\")\n","    pyplot.plot(truth[:500],color=\"blue\")\n","    pyplot.plot(test_result-truth,color=\"green\")\n","    pyplot.grid(True, which='both')\n","    pyplot.axhline(y=0, color='k')\n","#     pyplot.savefig('graph/transformer-epoch%d.png'%epoch)\n","    pyplot.close()\n","    return total_loss / i\n","\n","\n","# predict the next n steps based on the input data \n","def predict_future(eval_model, data_source,steps):\n","    eval_model.eval() \n","    total_loss = 0.\n","    test_result = torch.Tensor(0)    \n","    truth = torch.Tensor(0)\n","    data, _ = get_batch(data_source , 0 , 1)\n","    with torch.no_grad():\n","        for i in range(0, steps):            \n","            output = eval_model(data[-input_window:])\n","            # (seq-len , batch-size , features-num)\n","            # input : [ m,m+1,...,m+n ] -> [m+1,...,m+n+1]\n","            data = torch.cat((data, output[-1:])) # [m,m+1,..., m+n+1]\n","\n","    data = data.cpu().view(-1)\n","    \n","    # I used this plot to visualize if the model pics up any long therm structure within the data.\n","    pyplot.plot(data,color=\"red\")       \n","    pyplot.plot(data[:input_window],color=\"blue\")    \n","    pyplot.grid(True, which='both')\n","    pyplot.axhline(y=0, color='k')\n","#     pyplot.savefig('graph/transformer-future%d.png'%steps)\n","    pyplot.show()\n","    pyplot.close()\n","        \n","\n","def evaluate(eval_model, data_source):\n","    eval_model.eval() # Turn on the evaluation mode\n","    total_loss = 0.\n","    eval_batch_size = 1000\n","    with torch.no_grad():\n","        # for i in range(0, len(data_source) - 1, eval_batch_size): # Now len-1 is not necessary\n","        for i in range(0, len(data_source), eval_batch_size):\n","            data, targets = get_batch(data_source, i,eval_batch_size)\n","            output = eval_model(data)            \n","            total_loss += len(data[0]) * criterion(output, targets).cpu().item()\n","    return total_loss / len(data_source)\n","\n","train_data, val_data = get_data(\"/kaggle/input/vehicle-count/traffic.csv\", train_size, input_window, output_window, device)\n","model = JV2former().to(device)\n","\n","criterion = nn.L1Loss()\n","lr = 0.005 \n","#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n","\n","best_val_loss = float(\"inf\")\n","epochs = 10 # The number of epochs\n","best_model = None\n","\n","for epoch in range(1, epochs + 1):\n","    epoch_start_time = time.time()\n","    train(train_data)\n","    if ( epoch % 5 == 0 ):\n","        val_loss = plot_and_loss(model, val_data,epoch)\n","        #predict_future(model, val_data,200)\n","    else:\n","        val_loss = evaluate(model, val_data)\n","   \n","    print('-' * 89)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                     val_loss, math.exp(val_loss)))\n","    print('-' * 89)\n","\n","    #if val_loss < best_val_loss:\n","    #    best_val_loss = val_loss\n","    #    best_model = model\n","\n","    scheduler.step() \n","\n","#src = torch.rand(input_window, batch_size, 1) # (source sequence length,batch size,feature number) \n","#out = model(src)\n","#\n","#print(out)\n","#print(out.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4591720,"sourceId":7834045,"sourceType":"datasetVersion"},{"datasetId":4845206,"sourceId":8183278,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
